### k8s 1.21.5 添加 ceph 16 为为外部存储（动态存储卷）

#### k8s 节点上 安装 ceph-common
```cassandraql
yum install -y ceph-common
```

#### 1.动态持久卷
不需要存储管理员干预，使k8s使用的存储image创建自动化，即根据使用需要可以动态申请存储空间并自动创建。需要先定义一个或者多个StorageClass，每个StorageClass都必须配置一个provisioner，用来决定使用哪个卷插件分配PV。然后，StorageClass资源指定持久卷声明请求StorageClass时使用哪个provisioner来在对应存储创建持久卷。

#### 2.创建一个普通用户来给k8s做rdb的映射
在ceph集群中创建一个k8s专用的pool和用户：
```cassandraql
ceph osd pool create kube 1
ceph auth get-or-create client.kube mon 'allow r' osd 'allow class-read object_prefix rbd_children, allow rwx pool=kube' -o ceph.client.kube.keyring

```
#### 3.使用外部的rbd-provisioner来提供服务
如下操作再k8s的master上执行：
```cassandraql
git clone https://github.com/kubernetes-incubator/external-storage.git
kubectl apply -f ./external-storage/ceph/rbd/deploy/rbac
```
查看状态
```cassandraql
[root@master ~]#  kubectl describe deployments.apps  rbd-provisioner              
Name:               rbd-provisioner
Namespace:          default
CreationTimestamp:  Fri, 12 Aug 2022 18:39:25 +0800
Labels:             <none>
Annotations:        deployment.kubernetes.io/revision: 1
Selector:           app=rbd-provisioner
Replicas:           1 desired | 1 updated | 1 total | 1 available | 0 unavailable
StrategyType:       Recreate
MinReadySeconds:    0
Pod Template:
  Labels:           app=rbd-provisioner
  Service Account:  rbd-provisioner
  Containers:
   rbd-provisioner:
    Image:      quay.io/external_storage/rbd-provisioner:latest
    Port:       <none>
    Host Port:  <none>
    Environment:
      PROVISIONER_NAME:  ceph.com/rbd
    Mounts:              <none>
  Volumes:               <none>
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  <none>
NewReplicaSet:   rbd-provisioner-76f6bc6669 (1/1 replicas created)
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  61s   deployment-controller  Scaled up replica set rbd-provisioner-76f6bc6669 to 1
```


#### 4.在k8s集群创建kube用户的secret：
- 在ceph 中查看密钥
# ceph auth get-key client.kube|base64
```cassandraql

QVFBaEhQWmk3V3M2Q1JBQXRhd3hhdW8wa2hVeENYb3RJTWx1U3c9PQ==
```
- 在k8s 中创建
执行命令：vim ceph-kube-secret.yaml
```cassandraql
apiVersion: v1
kind: Secret
metadata:
  name: ceph-kube-secret
  namespace: default
data:
  key: QVFBaEhQWmk3V3M2Q1JBQXRhd3hhdW8wa2hVeENYb3RJTWx1U3c9PQ==
type:
  ceph.com/rbd

```
创建后执行：
```cassandraql
# kubectl create -f ceph-kube-secret.yaml
# kubectl get secret
NAME                  TYPE                                  DATA   AGE
ceph-kube-secret      kubernetes.io/rbd                     1      68s
```
创建 admin secret
在 ceph中执行：
```cassandraql
[root@ceph01 ~]# ceph auth get-key client.admin | base64
QVFCeDF2VmlJTm5qSmhBQVZ5VVJ2cWhtZFREeHVrNGpuWGdWa0E9PQ==
```

执行命令：vim ceph-secret.yaml
```cassandraql
apiVersion: v1
kind: Secret
metadata:
  name: ceph-secret
  namespace: default
data:
  key: QVFCeDF2VmlJTm5qSmhBQVZ5VVJ2cWhtZFREeHVrNGpuWGdWa0E9PQ==
type:
  ceph.com/rbd
```
创建后执行：
```cassandraql
# kubectl create -f ceph-secret.yaml
# kubectl get secret
NAME                  TYPE                                  DATA   AGE
ceph-secret      kubernetes.io/rbd                     1      68s
```

#### 5.创建StorageClass或者使用已经创建好的StorageClass
执行命令： vim StorageClass.yaml
```cassandraql
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: ceph-rbd
  annotations:
     storageclass.beta.kubernetes.io/is-default-class: "true"
provisioner: ceph.com/rbd
parameters:
  monitors: 172.31.96.70:6789,172.31.96.70:6789,172.31.96.70:6789
  adminId: admin
  adminSecretName: ceph-secret
  adminSecretNamespace: default
  pool: kube
  userId: kube
  userSecretName: ceph-kube-secret
  userSecretNamespace: default
  fsType: ext4
  imageFormat: "2"
  imageFeatures: "layering"
```
执行部署命令：
```cassandraql
[root@master ~]# kubectl apply -f StorageClass.yaml
storageclass.storage.k8s.io/ceph-rbd created
[root@master ~]# kubectl get storageclass
NAME                 PROVISIONER    RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE
ceph-rbd (default)   ceph.com/rbd   Delete          Immediate           false                  5s
```

主要指令使用说明如下：
1，storageclass.beta.kubernetes.io/is-default-class
如果设置为true，则为默认的storageclasss。pvc申请存储，如果没有指定storageclass，则从默认的storageclass申请。
2，adminId：ceph客户端ID，用于在ceph 池中创建映像。默认是 “admin”。
3，userId：ceph客户端ID，用于映射rbd镜像。默认与adminId相同。
4，imageFormat：ceph rbd镜像格式，“1” 或者 “2”。默认值是 “1”。
5，imageFeatures：这个参数是可选的，只能在你将imageFormat设置为 “2” 才使用。 目前支持的功能只是layering。默认是 “"，没有功能打开。


#### 6.创建持久卷声明
我们已经指定了默认的storageclass，故可以直接创建pvc。创建完成处于pending状态，当使用的时候才会触发provisioner创建:
```cassandraql
# vim pvc.yaml
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: ceph-sc-claim
spec:
  accessModes:
    - ReadWriteOnce
    - ReadOnlyMany
  resources:
    requests:
      storage: 10Mi

# kubectl apply -f pvc.yaml
persistentvolumeclaim/ceph-sc-claim created

# kubectl get pvc
NAME            STATUS    VOLUME     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
ceph-sc-claim   Pending                                        ceph-rbd       50s

```
